{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression example step to step\n",
    "\n",
    "Demostrate Logistic Regression algorithm steps by a simple dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "\n",
    "\n",
    "#from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from lr_utils import load_dataset\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will using this very simple dataset. Purposes of this notebook is learning how algorith calculates each step not for accuracy or performance of it**:\n",
    "\n",
    "<table style=\"width:25%\">\n",
    "  <tr>\n",
    "  <th>**m2**</th>\n",
    "  <th>**rooms**</th>\n",
    "  <th>**price > 100K**</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 100 </td>\n",
    "    <td> 3 </td> \n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> 45 </td>\n",
    "    <td> 2 </td>\n",
    "    <td> 0 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 20 </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 0 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 3\n",
      "Number of testing examples: m_test = 3\n",
      "Number of features: features_train = 2\n",
      "train_set_x_orig shape: (3, 2)\n",
      "train_set_y shape: (3,)\n",
      "test_set_x_orig shape: (3, 2)\n",
      "test_set_y shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "#Original dataset\n",
    "dataset=np.array([[100, 3, 1],[45, 2, 0],[20, 1, 0]])\n",
    "#Extract trainset from this\n",
    "train_set_x_orig = dataset[:,:2]\n",
    "train_set_y = dataset[:,-1]\n",
    "\n",
    "#CREATION OF VERY SIMPLE TEST SET\n",
    "test_dataset = np.array([[90,3,1],[80,2,1],[30,1,0],])\n",
    "test_set_x_orig = test_dataset[:,:2]\n",
    "test_set_y = test_dataset[:,-1]\n",
    "\n",
    "m_dataset, cols_dataset = dataset.shape\n",
    "m_test_dataset, cols_test_dataset = dataset.shape\n",
    "m_train, features_train = train_set_x_orig.shape\n",
    "m_test, features_test = test_set_x_orig.shape\n",
    "assert(m_dataset == m_train and features_train == cols_dataset - 1)\n",
    "assert(m_train == m_test_dataset and features_test == cols_test_dataset - 1)\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Number of features: features_train = \" + str(features_train))\n",
    "print (\"train_set_x_orig shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_orig shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2, 3)\n",
      "X_test shape: (2, 3)\n",
      "result of reshaping: \n",
      "[[100  45  20]\n",
      " [  3   2   1]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "X_train = train_set_x_orig.reshape(m_train,-1).T\n",
    "X_test = test_set_x_orig.reshape(m_test,-1).T\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "assert (X_train.shape[0] == features_train and X_train.shape[1] == m_train)\n",
    "assert (X_test.shape[0] == features_test and X_test.shape[1] == m_test)\n",
    "print (\"result of reshaping: \" )\n",
    "print (str(train_set_x_flatten[:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the scope of this example, we are not standarize dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define all intermediate functions (abstrac from it until understand manual calculation)**:\n",
    "\n",
    "- def sigmoid(z):\n",
    "    - Compute the sigmoid of z\n",
    "      Arguments:\n",
    "         z -- A scalar or numpy array of any size.\n",
    "      Return:\n",
    "         s -- sigmoid(z)\n",
    "    \n",
    "- def initialize_with_zeros(dim):\n",
    "    - This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "        Argument:\n",
    "          dim -- size of the w vector we want (or number of parameters in this case)\n",
    "          \n",
    "        Returns:\n",
    "          w -- initialized vector of shape (dim, 1)\n",
    "          b -- initialized scalar (corresponds to the bias)\n",
    "\n",
    "- def propagate(w, b, X, Y):\n",
    "    - Implement the cost function and its gradient for the propagation explained above\n",
    "      Arguments:\n",
    "        w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "        b -- bias, a scalar\n",
    "        X -- data of size (num_px * num_px * 3, number of examples)\n",
    "        Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "      Return:\n",
    "        cost -- negative log-likelihood cost for logistic regression\n",
    "        dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "        db -- gradient of the loss with respect to b, thus same shape as b\n",
    "\n",
    "- def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "     - This function optimizes w and b by running a gradient descent algorithm\n",
    "       Arguments:\n",
    "        w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "        b -- bias, a scalar\n",
    "        X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "        Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_cost -- True to print the loss every 100 steps\n",
    "      Returns:\n",
    "        params -- dictionary containing the weights w and bias b\n",
    "        grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "        costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "- def predict(w, b, X):\n",
    "    - Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "      Arguments:\n",
    "        w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "        b -- bias, a scalar\n",
    "        X -- data of size (num_px * num_px * 3, number of examples)\n",
    "      Returns:\n",
    "        Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    print(\"WT * X:\")\n",
    "    print(np.dot(w.T, X))\n",
    "    print(\"bias: \", b)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)             # compute activation\n",
    "    print(\"A:\")\n",
    "    print(A)\n",
    "    print(\"Y:\")\n",
    "    print(Y)\n",
    "    cost = np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / np.negative(m)    # compute cost\n",
    "        \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = (np.dot(X,(A - Y).T))/m\n",
    "    db = (np.sum(A - Y))/m\n",
    "    \n",
    "    print(\"dw:\")\n",
    "    print(dw)\n",
    "    print(\"db:\")\n",
    "    print(dw)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% iter nº {} %%%%%%%%%%%%%%%%%%%%%%%5\".format(i))\n",
    "\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        print(\"Acum w:\")\n",
    "        print(w)\n",
    "        print(\"Acum b:\")\n",
    "        print(b)\n",
    "        \n",
    "        # Record the costs\n",
    "        #if i % 100 == 0:\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost: #and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "def predict(w, b, X):\n",
    "    \n",
    "    print(\"w shape:\", w.shape)\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    print(\"X shape\", X.shape)\n",
    "    print(\"w reshape\", w.shape)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    \n",
    "    print(\"A shape: \", A.shape)\n",
    "    print(\"Y shape \", Y_prediction.shape)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "\n",
    "        if A[0,i]>=0.5:\n",
    "            Y_prediction[0,i] = 1.0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0.0\n",
    "\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression algorithm\n",
    "\n",
    "- Get X\n",
    "\n",
    "- Initialize de parameters w and b to zero.\n",
    "\n",
    "- Iterate to find minimun of cost **`for i:=1 to num_iterations repeat:`** \n",
    "\n",
    "    - Compute: \n",
    "        $$A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$$\n",
    "\n",
    "    - `For each` training set calculate the loss \n",
    "        $$L(a^i,y^i) = -[y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})] $$\n",
    "\n",
    "    - Calculate the cost function: \n",
    "        $$J = -\\frac{1}{m}\\sum_{i=1}^{m}L(a^i,y^i)$$\n",
    "    \n",
    "    - Calculate the derivatives of cost function respect w and b\n",
    "        $$ dw=\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{1}$$\n",
    "        $$ db=\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{2}$$\n",
    "    \n",
    "    - Update de parameters w and b, the update rule is ($\\alpha$ is the learning rate):\n",
    "        $$ w = w - \\alpha dw $$\n",
    "        $$ b = b - \\alpha db $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% iter nº 0 %%%%%%%%%%%%%%%%%%%%%%%5\n",
      "WT * X:\n",
      "[[ 0.  0.  0.]]\n",
      "bias:  0\n",
      "A:\n",
      "[[ 0.5  0.5  0.5]]\n",
      "Y:\n",
      "[1 0 0]\n",
      "dw:\n",
      "[[-5.83333333]\n",
      " [ 0.        ]]\n",
      "db:\n",
      "[[-5.83333333]\n",
      " [ 0.        ]]\n",
      "Acum w:\n",
      "array([[ 0.05833333],\n",
      "       [ 0.        ]])\n",
      "Acum b:\n",
      "-0.0016666666666666666\n",
      "Cost after iteration 0: 0.693147\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% iter nº 1 %%%%%%%%%%%%%%%%%%%%%%%5\n",
      "WT * X:\n",
      "[[ 5.83333333  2.625       1.16666667]]\n",
      "bias:  -0.00166666666667\n",
      "A:\n",
      "[[ 0.99707539  0.93234826  0.76224005]]\n",
      "Y:\n",
      "[1 0 0]\n",
      "dw:\n",
      "[[ 18.96933739]\n",
      " [  0.87272092]]\n",
      "db:\n",
      "[[ 18.96933739]\n",
      " [  0.87272092]]\n",
      "Acum w:\n",
      "array([[-0.13136004],\n",
      "       [-0.00872721]])\n",
      "Acum b:\n",
      "-0.0073055456906058851\n",
      "Cost after iteration 1: 1.377602\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION ALGORITHM\n",
    "\n",
    "#initialize parameters with zeros (≈ 1 line of code)\n",
    "w, b = initialize_with_zeros(features_train)\n",
    "\n",
    "num_iterations = 2\n",
    "learning_rate = 0.01\n",
    "print_cost = True\n",
    "parameters, grads, costs = optimize(w, b, train_set_x_flatten, train_set_y, num_iterations, learning_rate, print_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
